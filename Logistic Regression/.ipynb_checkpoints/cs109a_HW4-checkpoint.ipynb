{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science\n",
    "\n",
    "## Homework 4: Logistic Regression\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2019**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, and Chris Tanner\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- **This is an individual homework. No group collaboration.**\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- As much as possible, try and stick to the hints and functions we import at the top of the homework, as those are the ideas and tools the class supports and is aiming to teach. And if a problem specifies a particular library you're required to use that library, and possibly others from the import list.\n",
    "- Please use .head() when viewing data. Do not submit a notebook that is excessively long because output was not suppressed or otherwise limited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "import scipy\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import zipfile\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='theme'> Cancer Classification from Gene Expressions </div>\n",
    "\n",
    "In this problem, we will build a classification model to distinguish between two related classes of cancer, acute lymphoblastic leukemia (ALL) and acute myeloid leukemia (AML), using gene expression measurements. The data set is provided in the file `data/dataset_hw4.csv`. Each row in this file corresponds to a tumor tissue sample from a patient with one of the two forms of Leukemia. The first column contains the cancer type, with **0 indicating the ALL** class and **1 indicating the AML** class. Columns 2-7130 contain expression levels of 7129 genes recorded from each tissue sample. \n",
    "\n",
    "In the following questions, we will use linear and logistic regression to build classification models for this data set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 1 [20 pts]: Data Exploration </b></div>\n",
    "\n",
    "First step is to  split  the observations into an approximate 80-20 train-test split.  Below is some code to do this for you (we want to make sure everyone has the same splits). Print dataset shape before splitting and after splitting. `Cancer_type` is our target column.\n",
    "\n",
    "\n",
    "**1.1** Take a peek at your training set (show a glimpse of what you did): you should notice the severe differences in the measurements from one gene to the next (some are negative, some hover around zero, and some are well into the thousands).  To account for these differences in scale and variability, normalize each predictor to vary between 0 and 1.\n",
    "\n",
    "\n",
    "**1.2** The training set contains more predictors than observations. What problem(s) can this lead to in fitting a classification model to such a data set? Explain in 3 or fewer sentences.\n",
    "\n",
    "\n",
    "**1.3** Identify and report which 10 genes individually discriminate between the two cancer classes the best (consider every gene in the data set).  Note: it wil lbe useful to save this list for future parts.\n",
    "\n",
    "Plot two histograms ofyour best predictor - one using training and another for the testing dataset. Each histogram should clearly distinguish two different `Cancer_type` classes.\n",
    "\n",
    "Hint: You may use t-testing to make this determination: #https://en.wikipedia.org/wiki/Welch%27s_t-test.\n",
    "\n",
    "\n",
    "**1.4** Using your top gene from the previous part (call it  `best_predictor`), create a classification model by manually eye-balling a value for this gene that would discriminate the two classes the best. Justify your choice in 1-2 sentences. Report the accuracy of this hand-chosen model on the test set.\n",
    "\n",
    "<hr> <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First step is to split the observations into an approximate 80-20 train-test split. Below is some code to do this for you (we want to make sure everyone has the same splits). Print dataset shape before splitting and after splitting. `Cancer_type` is our target column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "df = pd.read_csv('data/hw4_enhance.csv', index_col=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test =train_test_split(df.loc[:, df.columns != 'Cancer_type'], \n",
    "                                                         df.Cancer_type, test_size=0.2, \n",
    "                                                         random_state = 109, \n",
    "                                                         stratify = df.Cancer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(df.Cancer_type.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Take a peek at your training set (show a glimpse of what you did): you should notice the severe differences in the measurements from one gene to the next (some are negative, some hover around zero, and some are well into the thousands).  To account for these differences in scale and variability, normalize each predictor to vary between 0 and 1.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3), dpi=100)\n",
    "sns.distplot(y_test , color=\"dodgerblue\", ax=axes[0], axlabel='Y Test')\n",
    "sns.distplot(y_train , color=\"deeppink\", ax=axes[1], axlabel='Y Train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_array = X_train.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(X_train_array)\n",
    "normalized_X_train = pd.DataFrame(x_scaled)\n",
    "normalized_X_train.columns = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 The training set contains more predictors than observations. What problem(s) can this lead to in fitting a classification model to such a data set? Explain in 3 or fewer sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The large dimensionality can mean that some of the columns may be related to the outcome variable by luck, rather than any kind of causal relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3** **Identify and report which 10 genes individually discriminate between the two cancer classes the best (consider every gene in the data set).  Note: it wil lbe useful to save this list for future parts.**\n",
    "\n",
    "**Plot two histograms of your best predictor - one using training and another for the testing dataset. Each histogram should clearly distinguish two different `Cancer_type` classes.**\n",
    "\n",
    "**Hint: You may use t-testing to make this determination: #https://en.wikipedia.org/wiki/Welch%27s_t-test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_test_results = {}\n",
    "AML = normalized_X_train.where(y_train== 1).dropna()\n",
    "ALL = normalized_X_train.where(y_train== 0).dropna()\n",
    "\n",
    "\n",
    "for x in normalized_X_train.columns:\n",
    "    AML2 = AML[:][x]\n",
    "    ALL2 = ALL[:][x]\n",
    "    t_test_results[x] = scipy.stats.ttest_ind(AML2,ALL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_train_df = pd.DataFrame.from_dict(t_test_results,orient='Index')\n",
    "t_test_train_df.columns = ['field','pvalue']\n",
    "t_test_train_df=t_test_train_df.sort_values('pvalue').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_10_pred = t_test_train_df.head(10).index\n",
    "top_10_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pred = t_test_train_df.head(1).index\n",
    "print(top_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 5))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "plt.subplot(1, 2,1)\n",
    "plt.scatter(X_train[top_pred],y_train)\n",
    "plt.title(\"Training Data\")\n",
    "plt.xlim(-600,1000)\n",
    "plt.xlabel(\"D26067_at Gene\")\n",
    "plt.yticks([0,1],['ALL Cancer Type','AML Cancer Type'])\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X_test[top_pred],y_test)\n",
    "plt.title(\"Test Data\")\n",
    "plt.xlim(-600,1000)\n",
    "plt.yticks([0,1],['ALL Cancer Type','AML Cancer Type'])\n",
    "plt.xlabel(\"D26067_at Gene\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 Using your top gene from the previous part (call it  `best_predictor`), create a classification model by eye-balling a value for this gene that would discriminate the two classes the best. Justify your choice in 1-2 sentences. Report the accuracy of this hand-chosen model on the test set.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a bit disconcerting that the test and training trends don't follow each other too closely, but there appears to be a common around 325. \n",
    "\n",
    "Given that, I would say that if the D26067_at gene is above 325 then I would think the person has ALL type of cancer. Below 325 and they would likely have AML type of cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 2 [25 pts]: Linear and Logistic Regression  </b></div>\n",
    "\n",
    "\n",
    "In class we discussed how to use both linear regression and logistic regression for classification. For this question, you will work with a single gene predictor that you identify as the best predictor above to explore these two methods.\n",
    "\n",
    "**2.1** Fit a simple linear regression model to the training set using the single gene predictor `best_predictor` to predict cancer type. The scores predicted by the regression model for a patient could be interpreted as an estimate of the probability that the patient has Cancer_type=1 (AML). Is there a problem with this interpretation?\n",
    "\n",
    "\n",
    "Create a figure with following items displayed on the same plot (use training data):\n",
    " - the predicted quantitative response from the linear regression model as a function of the best gene predictor\n",
    " - the true binary response. \n",
    "\n",
    "\n",
    "**2.2** Use your estimated linear regression model to classify observations into 0 and 1 using the standard Bayes' classifier.  Evaluate the classification accuracy of this classification model on both the training and test sets.\n",
    "\n",
    "\n",
    "**2.3** Next, fit a simple logistic regression model to the training set. How do the training and test classification accuracies of this model compare with the linear regression model? \n",
    "\n",
    "Remember, you need to set the regularization parameter for sklearn's logistic regression function to be a very large value in order to **not** regularize (use 'C=100000'). \n",
    "\n",
    "\n",
    "**2.4** \n",
    "Print and interpret the logistic regression coefficients: the 'slope' and intercept. \n",
    "\n",
    "\n",
    "Create 2 plots (one each for training and testing data) with 4 items displayed on each plot.\n",
    "- the predicted quantitative response from the linear regression model as a function of the best gene predictor.\n",
    "- the predicted probabilities of the logistic regression model as a function of the best gene predictor. \n",
    "- the true binary response. \n",
    "- a horizontal line at $y=0.5$. \n",
    "\n",
    "Based on these plots, does one of the models appear better suited for binary classification than the other?  Explain in 3 sentences or fewer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** **Fit a simple linear regression model to the training set using the single gene predictor `best_predictor` to predict cancer type. The scores predicted by the regression model for a patient could be interpreted as an estimate of the probability that the patient has Cancer_type=1 (AML).  Is there a problem with this interpretation?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# training set\n",
    "\n",
    "x_train_padded = sm.add_constant(X_train[top_pred]) # to allow for beta_0\n",
    "y_train_lr = y_train.values.reshape(-1,1)\n",
    "\n",
    "\n",
    "model = OLS(y_train_lr, x_train_padded)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the linear regression model may give probabilities above 1 or below 0, which would not make much intuitive sense (and also would not mean a guarantee that you do or do not have that type of cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** **Use your estimated linear regression model to classify observations into 0 and 1 using the standard Bayes' classifier.  Evaluate the classification accuracy of this classification model on both the training and test sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train_lin = results.predict(exog=x_train_padded)\n",
    "\n",
    "# development set\n",
    "x_dev_padded = sm.add_constant(X_test[top_pred])\n",
    "y_dev_lr = y_test.values.reshape(-1,1)\n",
    "\n",
    "y_hat_test_lin = results.predict(exog=x_dev_padded)\n",
    "\n",
    "print(\"Training accuracy: \", str(accuracy_score(y_train, np.round(y_hat_train_lin))))\n",
    "print(\"Test accuracy: \", str(accuracy_score(y_test, np.round(y_hat_test_lin))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**2.3** **Next, fit a simple logistic regression model to the training set. How do the training and test classification accuracies of this model compare with the linear regression model? Are the classifications substantially different?  Explain why this is the case.**\n",
    "\n",
    "**Remember, you need to set the regularization parameter for sklearn's logistic regression function to be a very large value in order to **not** regularize (use 'C=100000')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=100000000)\n",
    "lr.fit(X_train[top_pred], y_train)\n",
    "\n",
    "y_hat_train_log = lr.predict(X_train[top_pred])\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_hat_train_log))\n",
    "\n",
    "y_hat_test_log = lr.predict(X_test[top_pred])\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_hat_test_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "The training accuracy is a little bit worse and the test accuracy is the same. You might expect this because we are only using one variable, so there isn't likely to be a lot of variation between the different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Print and interpret the logistic regression coefficients: the 'slope' and the intercept.**\n",
    "\n",
    "**Create 2 plots (with training and test data) with 4 items displayed on each plot.**\n",
    "- the predicted quantitative response from the linear regression model as a function of the best gene predictor.\n",
    "- the predicted probabilities of the logistic regression model as a function of the best gene predictor. \n",
    "- the true binary response. \n",
    "- a horizontal line at $y=0.5$.\n",
    "\n",
    "**Based on these plots, does one of the models appear better suited for binary classification than the other?  Explain in 3 sentences or fewer.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "plt.subplot(1, 2,1)\n",
    "plt.scatter(X_train[top_pred],y_train,alpha=1,color='orange')\n",
    "plt.plot(X_train[top_pred],y_hat_train_lin)\n",
    "plt.scatter(X_train[top_pred],y_hat_train_log,alpha = 0.4)\n",
    "plt.hlines(xmin=-400,xmax=1000,y=0.5,color='r')\n",
    "\n",
    "\n",
    "plt.title(\"Training Data\")\n",
    "plt.xlim(-600,1000)\n",
    "plt.xlabel(\"D26067_at Gene\")\n",
    "plt.yticks([0,1],['ALL Cancer Type','AML Cancer Type'])\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X_test[top_pred],y_test,alpha=1,color='orange')\n",
    "plt.plot(X_test[top_pred],y_hat_test_lin)\n",
    "plt.scatter(X_test[top_pred],y_hat_test_log,alpha = 0.4)\n",
    "plt.hlines(xmin=-400,xmax=1000,y=0.5,color='r')\n",
    "plt.yticks([0,1],['ALL Cancer Type','AML Cancer Type'])\n",
    "plt.xlabel(\"D26067_at Gene\")\n",
    "\n",
    "\n",
    "plt.title(\"Test Data\")\n",
    "plt.xlim(-600,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the naked eye, they both look exactly the same, except the linear follows a slightly different path than the logistic. However, after we incorporate the cutoff after creating our predictions, I would expect a very similar result between the two models. Frankly, I'm a bit surprised the linear performed better for the training -- I would have thought they would have the same exact accuracy just like on the test dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 3 [20pts]: Multiple Logistic Regression </b> </div>\n",
    "\n",
    "\n",
    "**3.1** Next, fit a multiple logistic regression model with all the gene predictors from the data set.  How does the classification accuracy of this model compare with the models fitted in question 2 with a single gene (on both the training and test sets)?  \n",
    "\n",
    "\n",
    "**3.2** How many of the coefficients estimated by this multiple logistic regression in the previous part are significantly different from zero at a *significance level of 5%*? Use the same value of C=100000 as before.\n",
    "\n",
    "**Hint:** To answer this question, use *bootstrapping* with 100 bootstrap samples/iterations.  \n",
    "\n",
    "\n",
    "**3.3** Comment on the classification accuracy of training and test set? Given the results above how would you assess the generalization capacity of your trained model? What other tests would you suggest to better guard against false sense of security on the accuracy of the model as a whole? \n",
    "\n",
    "**3.4** Now use regularization to improve predictions from the multiple logistic regression model.  Use LASSO-like regularization and cross-validation within the training set to tune the model.  Report the classification accuracy on both the training and test set.\n",
    "\n",
    "**3.5** Do the 10 best predictors from Q1 hold up as important features in this regularized model?  If not, explain why this is the case (feel free to use the data to support your explanation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** **Next, fit a multiple logistic regression model with all the gene predictors from the data set.  How does the classification accuracy of this model compare with the models fitted in question 2 with a single gene (on both the training and test sets)?**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=100000000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_hat_train_log = lr.predict(X_train)\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_hat_train_log))\n",
    "\n",
    "y_hat_test_log = lr.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_hat_test_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "With all of the variables, we were able to completely fit the training data (not necessarily a good thing), and were able to get test accuracy up to 74%. This is a significant improvement, even if we are likely overfitting to our training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** **How many of the coefficients estimated by this multiple logistic regression in the previous part are significantly different from zero at a *significance level of 5%*? Use the same value of C=100000 as before.**\n",
    "\n",
    "**Hint:** **To answer this question, use *bootstrapping* with 1000 bootstrap samples/iterations.**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "coefficients = pd.DataFrame()\n",
    "\n",
    "for x in range(0,1000):\n",
    "    sample_index = np.random.choice(range(0, len(y_train)), len(y_train))\n",
    "\n",
    "    X_samples = X_train.iloc[sample_index]\n",
    "    y_samples = y_train.iloc[sample_index]    \n",
    "\n",
    "    lr = LogisticRegression(C=100000000)\n",
    "    lr.fit(X_samples, y_samples)\n",
    "    coefficients = pd.concat([coefficients,pd.DataFrame(lr.coef_)])\n",
    "    \n",
    "coefficients.columns = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "bootstrap_ttest = pd.DataFrame(columns = ['Field','Tset','P Value'])\n",
    "for col in coefficients.columns:\n",
    "    tset, pval = ttest_1samp(coefficients[col], 0)\n",
    "    bootstrap_ttest=pd.concat([bootstrap_ttest,pd.DataFrame({'Field':col,\n",
    "                                                             'Tset':tset,\n",
    "                                                            'P Value':pval},\n",
    "                                                           index=[0])\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_vals = bootstrap_ttest.loc[bootstrap_ttest['P Value']<0.05]\n",
    "print(len(significant_vals),\" variables are significantly different from 0 at 5% significance level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Open question: Comment on the classification accuracy of training and test set? Given the results above how would you assest the generalization capacity of your trained model? What other tests would you suggest to better guard against false sense of security on the accuracy of the model as a whole.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "Our logistic regression model was able to completely fit the dataset, which can be expected when there are more predictors than there are observations. This probably isn't a good thing. I think it would be a good idea to either bootstrap or find some way to remove features that might be extra noise for our dataset. We should aim to have less features than we have observations to avoid a perfect fit on the training data. If this isn't possible we can do a lot of bootstrapping to help as well. \n",
    "\n",
    "In terms of other tests, we could remove ~10% of our training data to create a validation group to help us understand the accuracy of the model before we look at the actual test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4 Now use regularization to improve predictions from the multiple logistic regression model.  Use LASSO-like regularization and cross-validation within the training set to tune the model.  Report the classification accuracy on both the training and test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lasso = Lasso(random_state=0, max_iter=10000)\n",
    "model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_lasso = model_lasso.predict(X_test)\n",
    "y_hat_train_lasso = model_lasso.predict(X_train)\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, np.round(y_hat_train_lasso)))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, np.round(y_hat_test_lasso)))\n",
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5: Do the 10 best predictors from Q1 hold up as important features in this regularized model?  If not, explain why this is the case (feel free to use the data to support your explanation).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in top_10_pred:\n",
    "    print(x,\": \",coef[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The lasso regression made 9 of the top 10 predictors into non-factors for the final model. This suggests that the variables were not the best at differentiating the cancer types. It is interesting to note that almost all of the features showed a statistically significant coefficient when running the logistic regression bootstrapping model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 4 [25pts]: Multi Class Log Regression </b> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** Load the data from `hw4_mc_enhance.csv.zip` and examine its structure. How many instances of each class are there in our dataset? \n",
    "\n",
    "**4.2** Split the dataset into train and test, 80-20 split, random_state = 8. \n",
    "\n",
    "We are going to utilize these two features - 'M31523_at', 'X95735_at'.  Create a scatter plot of these two features using the training dataset.  It should be easily discernable via labeling/marking in the plot which observations belong to which `cancer_type`.\n",
    "\n",
    "**4.3** Fit the following two models using crossvalidation: \n",
    "- Logistic Regression Multiclass model with linear features. \n",
    "- Logistic Regression Multiclass model with Polynomial features, degree = 2.\n",
    "\n",
    "**4.4** Plot the decision boundaries for each model and interpret the results. Hint: You may utilize the function `overlay_decision_boundary`. \n",
    "\n",
    "**4.5** Report and plot the CV scores for the two models and interpret. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1 Load the data from `hw4_mc_enhance.csv.zip` and examine its structure. How many instances of each class are there in our dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_enhance = pd.read_csv(\"./data/hw4_mc_enhance.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_enhance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_enhance.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the enhanced data, there are 3 classes of cancer type (0,1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 Split the dataset into train and test, 80-20 split, random_state = 8.**\n",
    "\n",
    "**We are going to utilize these two features - 'M31523_at', 'X95735_at'.  Create a scatter plot of these two features using training dataset.  We should be able to discern from the plot which sample belongs to which `cancer_type`.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 =train_test_split(mc_enhance.loc[:, mc_enhance.columns != 'cancer_type'], \n",
    "                                                         mc_enhance.cancer_type, test_size=0.2, \n",
    "                                                         random_state = 8, \n",
    "                                                         stratify = mc_enhance.cancer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "plt.subplot(1, 2,1)\n",
    "plt.scatter(X_train2['M31523_at'],y_train2,alpha=1,color='orange')\n",
    "\n",
    "plt.title(\"M31523_at Data\")\n",
    "plt.ylabel(\"M31523_at Gene\")\n",
    "plt.xlabel(\"Cancer Type\")\n",
    "plt.yticks([0,1,2])\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X_train2['X95735_at'],y_train2,alpha=1,color='orange')\n",
    "plt.yticks([0,1,2])\n",
    "\n",
    "plt.title(\"X95735_at Data\")\n",
    "plt.ylabel(\"X95735_at Gene\")\n",
    "plt.xlabel(\"Cancer Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3  Fit the following two models using crossvalidation:**\n",
    "\n",
    "**Logistic Regression Multiclass model with linear features.**\n",
    "\n",
    "**Logistic Regression Multiclass model with Polynomial features, degree = 2.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2_subset = X_train2[['M31523_at','X95735_at']]\n",
    "X_test2_subset = X_test2[['M31523_at','X95735_at']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "mc_enhance_logistic_model = linear_model.LogisticRegressionCV(cv=5,multi_class='multinomial', \n",
    "                                     random_state=8).fit(X_train2_subset,\n",
    "                                                                  y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2_subset_poly['M31523_at'] = X_train2_subset['M31523_at']**2\n",
    "X_train2_subset_poly['X95735_at'] = X_train2_subset['X95735_at']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_enhance_poly_logistic_model = linear_model.LogisticRegressionCV(cv=5,multi_class='multinomial', \n",
    "                                     random_state=8).fit(X_train2_subset_poly, y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4 Plot the decision boundary and interpret results. Hint: You may utilize the function `overlay_decision_boundary`** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_enhance_poly_logistic_model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_decision_boundary(ax, model, colors=None, nx=200, \n",
    "                              ny=200, desaturate=.5, xlim=None, ylim=None):\n",
    "    \"\"\"\n",
    "    A function that visualizes the decision boundaries of a classifier.\n",
    "    \n",
    "    ax: Matplotlib Axes to plot on\n",
    "    model: Classifier to use.\n",
    "     - if `model` has a `.predict` method, like an sklearn classifier, we call `model.predict(X)`\n",
    "     - otherwise, we simply call `model(X)`\n",
    "    colors: list or dict of colors to use. Use color `colors[i]` for class i.\n",
    "     - If colors is not provided, uses the current color cycle\n",
    "    nx, ny: number of mesh points to evaluated the classifier on\n",
    "    desaturate: how much to desaturate each of the colors (for better contrast with the sample points)\n",
    "    xlim, ylim: range to plot on. (If the default, None, is passed, the limits will be taken from `ax`.)\n",
    "    \"\"\"\n",
    "    # Create mesh.\n",
    "    xmin, xmax = ax.get_xlim() if xlim is None else xlim\n",
    "    ymin, ymax = ax.get_ylim() if ylim is None else ylim\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(xmin, xmax, nx),\n",
    "        np.linspace(ymin, ymax, ny))\n",
    "    X = np.c_[xx.flatten(), yy.flatten()]\n",
    "\n",
    "    # Predict on mesh of points.\n",
    "    model = getattr(model, 'predict', model)\n",
    "    y = model(X)\n",
    "    #print(\"Do I predict\" , y)\n",
    "    \n",
    "    \n",
    "#     y[np.where(y=='aml')]=3\n",
    "#     y[np.where(y=='allT')]=2\n",
    "#     y[np.where(y=='allB')]=1\n",
    "    \n",
    "    \n",
    "    \n",
    "    y = y.astype(int) # This may be necessary for 32-bit Python.\n",
    "    y = y.reshape((nx, ny))\n",
    "\n",
    "    # Generate colormap.\n",
    "    if colors is None:\n",
    "        # If colors not provided, use the current color cycle.\n",
    "        # Shift the indices so that the lowest class actually predicted gets the first color.\n",
    "        # ^ This is a bit magic, consider removing for next year.\n",
    "        colors = (['white'] * np.min(y)) + sns.utils.get_color_cycle()\n",
    "\n",
    "    if isinstance(colors, dict):\n",
    "        missing_colors = [idx for idx in np.unique(y) if idx not in colors]\n",
    "        assert len(missing_colors) == 0, f\"Color not specified for predictions {missing_colors}.\"\n",
    "\n",
    "        # Make a list of colors, filling in items from the dict.\n",
    "        color_list = ['white'] * (np.max(y) + 1)\n",
    "        for idx, val in colors.items():\n",
    "            color_list[idx] = val\n",
    "    else:\n",
    "        assert len(colors) >= np.max(y) + 1, \"Insufficient colors passed for all predictions.\"\n",
    "        color_list = colors\n",
    "    color_list = [sns.utils.desaturate(color, desaturate) for color in color_list]\n",
    "    cmap = matplotlib.colors.ListedColormap(color_list)\n",
    "\n",
    "    # Plot decision surface\n",
    "    ax.pcolormesh(xx, yy, y, zorder=-2, cmap=cmap, norm=matplotlib.colors.NoNorm(), vmin=0, vmax=y.max() + 1)\n",
    "    xx = xx.reshape(nx, ny)\n",
    "    yy = yy.reshape(nx, ny)\n",
    "    if len(np.unique(y)) > 1:\n",
    "        ax.contour(xx, yy, y, colors=\"black\", linewidths=1, zorder=-1)\n",
    "    else:\n",
    "        print(\"Warning: only one class predicted, so not plotting contour lines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Multinomial Regression Decision Overlay Graph\")\n",
    "overlay_decision_boundary(ax=plt.axes([1,1,1,1], projection=None, polar=False), \n",
    "                              model=mc_enhance_logistic_model, colors=None, nx=200, \n",
    "                              ny=200, desaturate=.5, xlim=[-2000,8000], ylim=[-2000,14000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Polynomial Logistic Multinomial Regression Decision Overlay Graph\")\n",
    "overlay_decision_boundary(ax=plt.axes([1,1,1,1], projection=None, polar=False), \n",
    "                              model=mc_enhance_poly_logistic_model, colors=None, nx=200, \n",
    "                              ny=200, desaturate=.5, xlim=[-2000,8000], ylim=[-2000,14000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.5 Report and plot the CV scores for the two models and interpret.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_poly = []\n",
    "cv_scores = []\n",
    "\n",
    "for x in mc_enhance_poly_logistic_model.scores_[0]:\n",
    "    cv_scores_poly.append(x[0])\n",
    "\n",
    "for x in mc_enhance_logistic_model.scores_[0]:\n",
    "    cv_scores.append(x[0])\n",
    "\n",
    "\n",
    "plt.plot(cv_scores,label = \"Non-Transformed Logistic Regression Model\")\n",
    "plt.plot(cv_scores_poly,label = \"Polynomial Transformed Logistic Regression Model\")\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel(\"Cross Validation\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks([0,1,2,3,4])\n",
    "plt.yticks([0,0.25,0.5,0.75, 1])\n",
    "plt.title(\"CV Scores -- Logistic Regression Model Comparison\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "Each validation set did a little bit differently but were all generally in the same score range. The polynomial-transformed regression model did worse than the non-transformed dataset when looking at the training dataset. Realistically, we'd want to see how these performed on the test dataset before jumping to any conclusions. \n",
    "\n",
    "One serious consideration is around the polynomial transformation process. In this, I just used the squared versions of the features, when we'd probably want the non-squared variables and maybe even interaction versions as well for our transformed regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 5: [10 pts] Including an 'abstain' option </b></div>\n",
    "\n",
    "One of the reasons a hospital might be hesitant to use your cancer classification model is that a misdiagnosis by the model on a patient can sometimes prove to be very costly (e.g. if the patient were to file a law suit seeking a compensation for damages). One way to mitigate this concern is to allow the model to 'abstain' from making a prediction: whenever it is uncertain about the diagnosis for a patient. However, when the model abstains from making a prediction, the hospital will have to forward the patient to a specialist, which would incur additional cost.  How could one design a cancer classification model with an abstain option, such that the cost to the hospital is minimized?\n",
    "\n",
    "*Hint:* Think of ways to build on top of the logistic regression model and have it abstain on patients who are difficult to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1** More specifically, suppose the cost incurred by a hospital when a model mis-predicts on a patient is $\\$5000$ , and the cost incurred when the model abstains from making a prediction is \\$1000. What is the average cost per patient for the OvR logistic regression model (without quadratic or interaction terms) from **Question 4**.  Note that this needs to be evaluated on the patients in the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2** Design a classification strategy (into the 3 groups plus the *abstain* group) that has as low cost as possible per patient (certainly lower cost per patient than the logistic regression model).   Give a justification for your approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1 More specifically, suppose the cost incurred by a hospital when a model mis-predicts on a patient is $\\$5000$ , and the cost incurred when the model abstains from making a prediction is \\$1000. What is the average cost per patient for the OvR logistic regression model (without quadratic or interaction terms) from Question 4.  Note that this needs to be evaluated on the patients in the test set.**\n",
    "**\n",
    "...\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train_enhance=mc_enhance_logistic_model.predict(X_train2_subset)\n",
    "y_hat_test_enhance=mc_enhance_logistic_model.predict(X_test2_subset)\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train2, np.round(y_hat_train_enhance)))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test2, np.round(y_hat_test_enhance)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "array = confusion_matrix(y_test2, np.round(y_hat_test_enhance))\n",
    "df_cm = pd.DataFrame(array, index = [i for i in '012'],\n",
    "                  columns = [i for i in \"012\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.title(\"Confusion Matrix of Cancer Types\")\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.xlabel(\"Correct Class\")\n",
    "plt.ylabel(\"Predicted Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "92% correct is not too bad all things considered. 12 incorrect out of 150 would give an average cost of $400 per patient if we misdiagnosed the 12 and used the logistic regression outright. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2 Design a classification strategy (into the 3 groups plus the *abstain* group) that has as low cost as possible per patient (certainly lower cost per patient than the logistic regression model).   Give a justification for your approach.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The simplest action would be to predict probabilities for each of the classes and abstain from predictions where the probability is not above a certain threshold. On the costs, abstaining 5 times is the same as getting 1 mis-diagnosis. To make a true determination, we'd also want to know how much money we save on an accurate diagnosis. If we know these 3 things then we can start adjusting our probability criteria to optimize between these 3 considerations. Also, looking at the confusion matrix above, we may be able to target our issue areas by looking at common misclassifications. For example, our biggest mistakes are between 0's and 1's, but we aren't misclassifying any 2's as 0's. So we probably should keep our cutoff the same for 2's, but 1's and 0's should likely have an abstain range. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
